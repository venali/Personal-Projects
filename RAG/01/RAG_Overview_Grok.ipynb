{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqpsYn49QWSR"
   },
   "source": [
    "#Introducing Naive, Advanced, and Modular RAG\n",
    "\n",
    "**Implemented with xAI Grok-beta**\n",
    "\n",
    "This notebook introduces Naïve, Advanced, and Modular RAG through basic educational examples.\n",
    "\n",
    "The Naïve, Advanced and modular RAG techniques offer flexibility in selecting retrieval strategies, allowing adaptation to various tasks and data characteristics.\n",
    "\n",
    "**Summary**\n",
    "\n",
    "**Part 1: Foundations and Basic Implementation**\n",
    "\n",
    "1.Environment setup for OpenAI API integration  \n",
    "2.Generator function using GPT models    \n",
    "3.Dataetup with a list of documents (db_records)  \n",
    "4.Query(user request)  \n",
    "\n",
    "**Part 2: Advanced Techniques and Evaluation**\n",
    "\n",
    "1.Retrieval metrics  \n",
    "2.Naive RAG  \n",
    "3.Advanced RAG  \n",
    "4.Modular RAG Retriever  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ICSQQ0ipxlR"
   },
   "source": [
    "# Part 1: Foundations and Basic Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o01-IM8bTc5f"
   },
   "source": [
    "# 1.The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "myXJn33zbqTR",
    "outputId": "6554c940-debd-40e2-93a3-fb462f760ddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#API Key\n",
    "#Store you key in a file and read it(you can type it directly in the notebook but it will be visible for somebody next to you)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oefvqp21Ba07"
   },
   "outputs": [],
   "source": [
    "f = open(\"drive/MyDrive/files/API_Grok.txt\", \"r\")\n",
    "xAI_KEY=f.readline().strip()\n",
    "f.close()\n",
    "\n",
    "#The xAI Key\n",
    "import os\n",
    "os.environ['xAI_KEY'] =xAI_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuZ7jr4Rs36U"
   },
   "source": [
    "# 2.The Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwCNTW9fs36U"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "def call_llm_with_full_text(itext):\n",
    "    # Set the URL and headers\n",
    "    url = \"https://api.x.ai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {os.getenv('xAI_KEY')}\"\n",
    "    }\n",
    "\n",
    "    # Prepare the data to be sent with the request\n",
    "    data = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a test assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": itext\n",
    "            }\n",
    "        ],\n",
    "        \"model\": \"grok-beta\",\n",
    "        \"stream\": False,\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "\n",
    "    # Send the POST request\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "    # Extract the response content\n",
    "    if response.status_code == 200:\n",
    "        # Convert the response to a dictionary\n",
    "        response_data = response.json()\n",
    "        # Extracting the content of the assistant response\n",
    "        content = response_data['choices'][0]['message']['content']\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        content = \"\"\n",
    "\n",
    "    # Return the extracted text content\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVKe9VF0HIHJ"
   },
   "source": [
    "## Formatted response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oG8I2Kb2HFhL"
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_formatted_response(response):\n",
    "    # Define the width for wrapping the text\n",
    "    wrapper = textwrap.TextWrapper(width=80)  # Set to 80 columns wide, but adjust as needed\n",
    "    wrapped_text = wrapper.fill(text=response)\n",
    "\n",
    "    # Print the formatted response with a header and footer\n",
    "    print(\"Response:\")\n",
    "    print(\"---------------\")\n",
    "    print(wrapped_text)\n",
    "    print(\"---------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qv1ExPiZdJRL"
   },
   "source": [
    " # 3.The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45CFxG4Fgcju"
   },
   "outputs": [],
   "source": [
    "db_records = [\n",
    "    \"Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).\",\n",
    "    \"It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.\",\n",
    "    \"This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.\",\n",
    "    \"At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).\",\n",
    "    \"This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.\",\n",
    "    \"Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.\",\n",
    "    \"This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.\",\n",
    "    \"The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.\",\n",
    "    \"This component merges the outputs from the language model and the retrieval system.\",\n",
    "    \"It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.\",\n",
    "    \"The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.\",\n",
    "    \"When a query or prompt is received, the system first processes it to understand the requirement or the context.\",\n",
    "    \"Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.\",\n",
    "    \"This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.\",\n",
    "    \"The retrieved documents are then fed into the language model.\",\n",
    "    \"In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.\",\n",
    "    \"The language model, now augmented with direct access to retrieved information, generates a response.\",\n",
    "    \"This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.\",\n",
    "    \"By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.\",\n",
    "    \"This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.\",\n",
    "    \"Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.\",\n",
    "    \"This allows them to remain current with the latest knowledge and trends without needing frequent retraining.\",\n",
    "    \"With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.\",\n",
    "    \"While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.\",\n",
    "    \"These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.\",\n",
    "    \"Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.\",\n",
    "    \"In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.\",\n",
    "    \"A RAG vector store is a database or dataset that contains vectorized data points.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FIQ7NK92g7EC",
    "outputId": "eb7be7c0-d09b-4939-dba0-84723128fed5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach\n",
      "in the field of artificial intelligence, particularly within the realm of\n",
      "natural language processing (NLP). It innovatively combines the capabilities of\n",
      "neural network-based language models with retrieval systems to enhance the\n",
      "generation of text, making it more accurate, informative, and contextually\n",
      "relevant. This methodology leverages the strengths of both generative and\n",
      "retrieval architectures to tackle complex tasks that require not only linguistic\n",
      "fluency but also factual correctness and depth of knowledge. At the core of\n",
      "Retrieval Augmented Generation (RAG) is a generative model, typically a\n",
      "transformer-based neural network, similar to those used in models like GPT\n",
      "(Generative Pre-trained Transformer) or BERT (Bidirectional Encoder\n",
      "Representations from Transformers). This component is responsible for producing\n",
      "coherent and contextually appropriate language outputs based on a mixture of\n",
      "input prompts and additional information fetched by the retrieval component.\n",
      "Complementing the language model is the retrieval system, which is usually built\n",
      "on a database of documents or a corpus of texts. This system uses techniques\n",
      "from information retrieval to find and fetch documents that are relevant to the\n",
      "input query or prompt. The mechanism of relevance determination can range from\n",
      "simple keyword matching to more complex semantic search algorithms which\n",
      "interpret the meaning behind the query to find the best matches. This component\n",
      "merges the outputs from the language model and the retrieval system. It\n",
      "effectively synthesizes the raw data fetched by the retrieval system into the\n",
      "generative process of the language model. The integrator ensures that the\n",
      "information from the retrieval system is seamlessly incorporated into the final\n",
      "text output, enhancing the model's ability to generate responses that are not\n",
      "only fluent and grammatically correct but also rich in factual details and\n",
      "context-specific nuances. When a query or prompt is received, the system first\n",
      "processes it to understand the requirement or the context. Based on the\n",
      "processed query, the retrieval system searches through its database to find\n",
      "relevant documents or information snippets. This retrieval is guided by the\n",
      "similarity of content in the documents to the query, which can be determined\n",
      "through various techniques like vector embeddings or semantic similarity\n",
      "measures. The retrieved documents are then fed into the language model. In some\n",
      "implementations, this integration happens at the token level, where the model\n",
      "can access and incorporate specific pieces of information from the retrieved\n",
      "texts dynamically as it generates each part of the response. The language model,\n",
      "now augmented with direct access to retrieved information, generates a response.\n",
      "This response is not only influenced by the training of the model but also by\n",
      "the specific facts and details contained in the retrieved documents, making it\n",
      "more tailored and accurate. By directly incorporating information from external\n",
      "sources, Retrieval Augmented Generation (RAG) models can produce responses that\n",
      "are more factual and relevant to the given query. This is particularly useful in\n",
      "domains like medical advice, technical support, and other areas where precision\n",
      "and up-to-date knowledge are crucial. Retrieval Augmented Generation (RAG)\n",
      "systems can dynamically adapt to new information since they retrieve data in\n",
      "real-time from their databases. This allows them to remain current with the\n",
      "latest knowledge and trends without needing frequent retraining. With access to\n",
      "a wide range of documents, Retrieval Augmented Generation (RAG) systems can\n",
      "provide detailed and nuanced answers that a standalone language model might not\n",
      "be capable of generating based solely on its pre-trained knowledge. While\n",
      "Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes\n",
      "with its challenges. These include the complexity of integrating retrieval and\n",
      "generation systems, the computational overhead associated with real-time data\n",
      "retrieval, and the need for maintaining a large, up-to-date, and high-quality\n",
      "database of retrievable texts. Furthermore, ensuring the relevance and accuracy\n",
      "of the retrieved information remains a significant challenge, as does managing\n",
      "the potential for introducing biases or errors from the external sources. In\n",
      "summary, Retrieval Augmented Generation represents a significant advancement in\n",
      "the field of artificial intelligence, merging the best of retrieval-based and\n",
      "generative technologies to create systems that not only understand and generate\n",
      "natural language but also deeply comprehend and utilize the vast amounts of\n",
      "information available in textual form. A RAG vector store is a database or\n",
      "dataset that contains vectorized data points.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "paragraph = ' '.join(db_records)\n",
    "wrapped_text = textwrap.fill(paragraph, width=80)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aL7cHuuLhQ5w"
   },
   "source": [
    "# 4.The Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qARk6gtohSXW"
   },
   "outputs": [],
   "source": [
    "query = \"What store offers rags?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iITo3QIF7yeK"
   },
   "source": [
    "Generation without augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TXBILWI47yeM",
    "outputId": "0e9170b6-47fd-4d02-dedb-19e9b187d413"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "Many stores offer rags or cleaning cloths. Here are some places where you can\n",
      "typically find them:  1. **Hardware Stores** - Like Home Depot, Lowe's, or Ace\n",
      "Hardware. 2. **Supermarkets** - Most grocery stores have a cleaning supplies\n",
      "section where you can find rags or microfiber cloths. 3. **Big Box Retailers** -\n",
      "Stores like Walmart or Target often have a wide selection of cleaning supplies.\n",
      "4. **Dollar Stores** - Dollar Tree, Family Dollar, or similar stores often carry\n",
      "inexpensive rags. 5. **Automotive Stores** - For more heavy-duty rags, places\n",
      "like AutoZone or O'Reilly Auto Parts might have what you need. 6. **Online\n",
      "Retailers** - Amazon, eBay, or even directly from manufacturers' websites. 7.\n",
      "**Specialty Cleaning Supply Stores** - These might offer professional-grade rags\n",
      "or cloths.  When looking for rags, consider what you'll be using them for: -\n",
      "**Microfiber cloths** are great for dusting and cleaning without leaving\n",
      "streaks. - **Cotton rags** or **terry cloth** are good for general cleaning and\n",
      "can be washed and reused. - **Shop rags** are often used for automotive work or\n",
      "heavy-duty cleaning.  Remember to check for sales or bulk deals if you need a\n",
      "large quantity.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function and print the result\n",
    "llm_response = call_llm_with_full_text(query)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11HLkKQMqaDt"
   },
   "source": [
    "# Part 2: Advanced Techniques and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMGuZg1WiaUE"
   },
   "source": [
    "# 1.Retrieval Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHN6s7wZirQL"
   },
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GLECrTQirQN"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_cosine_similarity(text1, text2):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        use_idf=True,\n",
    "        norm='l2',\n",
    "        ngram_range=(1, 2),  # Use unigrams and bigrams\n",
    "        sublinear_tf=True,   # Apply sublinear TF scaling\n",
    "        analyzer='word'      # You could also experiment with 'char' or 'char_wb' for character-level features\n",
    "    )\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
    "    return similarity[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTJOi-jrjI5A"
   },
   "source": [
    "## Enhanced Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cnmPG9UWjJXD",
    "outputId": "695a0f09-c842-461e-c728-093eb6004fe5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    lemmatized_words = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        lemmatized_words.append(token.lemma_)\n",
    "    return lemmatized_words\n",
    "\n",
    "def expand_with_synonyms(words):\n",
    "    expanded_words = words.copy()\n",
    "    for word in words:\n",
    "        expanded_words.extend(get_synonyms(word))\n",
    "    return expanded_words\n",
    "\n",
    "def calculate_enhanced_similarity(text1, text2):\n",
    "    # Preprocess and tokenize texts\n",
    "    words1 = preprocess_text(text1)\n",
    "    words2 = preprocess_text(text2)\n",
    "\n",
    "    # Expand with synonyms\n",
    "    words1_expanded = expand_with_synonyms(words1)\n",
    "    words2_expanded = expand_with_synonyms(words2)\n",
    "\n",
    "    # Count word frequencies\n",
    "    freq1 = Counter(words1_expanded)\n",
    "    freq2 = Counter(words2_expanded)\n",
    "\n",
    "    # Create a set of all unique words\n",
    "    unique_words = set(freq1.keys()).union(set(freq2.keys()))\n",
    "\n",
    "    # Create frequency vectors\n",
    "    vector1 = [freq1[word] for word in unique_words]\n",
    "    vector2 = [freq2[word] for word in unique_words]\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    vector1 = np.array(vector1)\n",
    "    vector2 = np.array(vector2)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFqh9rr81SUn"
   },
   "source": [
    "# 2.Naive RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wu8vteKmS_qO"
   },
   "source": [
    "## Keyword search and matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WY1JU0Ush_l4",
    "outputId": "5d5dc06c-7ae0-42ae-ac4e-bc84097f2bbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Keyword Score: 1\n",
      "Response:\n",
      "---------------\n",
      "While Retrieval Augmented Generation (RAG) offers substantial benefits, it also\n",
      "comes with its challenges.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_best_match_keyword_search(query, db_records):\n",
    "    best_score = 0\n",
    "    best_record = None\n",
    "\n",
    "    # Split the query into individual keywords\n",
    "    query_keywords = set(query.lower().split())\n",
    "\n",
    "    # Iterate through each record in db_records\n",
    "    for record in db_records:\n",
    "        # Split the record into keywords\n",
    "        record_keywords = set(record.lower().split())\n",
    "\n",
    "        # Calculate the number of common keywords\n",
    "        common_keywords = query_keywords.intersection(record_keywords)\n",
    "        current_score = len(common_keywords)\n",
    "\n",
    "        # Update the best score and record if the current score is higher\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_record = record\n",
    "\n",
    "    return best_score, best_record\n",
    "\n",
    "# Assuming 'query' and 'db_records' are defined in previous cells in your Colab notebook\n",
    "best_keyword_score, best_matching_record = find_best_match_keyword_search(query, db_records)\n",
    "\n",
    "print(f\"Best Keyword Score: {best_keyword_score}\")\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oak-3k_dkzC3"
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "blcPOIaHkzC4",
    "outputId": "13cbe267-405d-4c50-cd02-c2a43728cacd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cosine Similarity Score: 0.059\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity\n",
    "score = calculate_cosine_similarity(query, best_matching_record)\n",
    "print(f\"Best Cosine Similarity Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1OW0l24IkzC5",
    "outputId": "bc609545-0532-4bc1-c0a7-5acfbfd55c18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What store offers rags? :  While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.\n",
      "Enhanced Similarity:, 0.641\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Similarity\n",
    "response = best_matching_record\n",
    "print(query,\": \", response)\n",
    "similarity_score = calculate_enhanced_similarity(query, response)\n",
    "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2zKQhiO0Fcr"
   },
   "source": [
    "## Augmented input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_7ymSxG0Fcs"
   },
   "outputs": [],
   "source": [
    "augmented_input=query+ \": \"+ best_matching_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7NTfxzum0PT2",
    "outputId": "dfd5385f-73b0-4e63-f4b2-1a5b873fd7a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "What store offers rags?: While Retrieval Augmented Generation (RAG) offers\n",
      "substantial benefits, it also comes with its challenges.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(augmented_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ui8wH4k3_g4"
   },
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jh8BsnUy0Fcs",
    "outputId": "b622f91f-6b5e-4421-bd5d-d883851200a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "Here are some of the **challenges** associated with Retrieval Augmented\n",
      "Generation (RAG):  1. **Complexity in Integration**: Integrating RAG into\n",
      "existing systems can be complex due to the need for:    - A robust retrieval\n",
      "system.    - A language model capable of understanding and generating\n",
      "contextually relevant responses.    - Mechanisms to ensure the seamless\n",
      "interaction between retrieval and generation components.  2. **Quality of\n",
      "Retrieval**:     - **Relevance**: Ensuring that the retrieved documents or data\n",
      "are relevant to the query can be challenging. Irrelevant or outdated information\n",
      "can lead to poor responses.    - **Efficiency**: Retrieval must be fast to\n",
      "maintain real-time interaction, which can be difficult with large datasets.  3.\n",
      "**Data Management**:    - **Data Volume**: Managing and indexing large volumes\n",
      "of data for quick retrieval is resource-intensive.    - **Data Quality**: The\n",
      "system's performance heavily depends on the quality of the data. Poorly curated\n",
      "data can degrade performance.  4. **Model Bias and Fairness**:    - RAG systems\n",
      "can inherit biases from both the retrieval system and the language model,\n",
      "potentially leading to biased outputs.  5. **Scalability**:    - As the amount\n",
      "of data grows, scaling the retrieval system to maintain performance can be\n",
      "challenging.  6. **Latency**:    - The additional step of retrieval adds to the\n",
      "latency of the system, which might not be suitable for applications requiring\n",
      "instant responses.  7. **Contextual Understanding**:    - Ensuring that the\n",
      "model understands the context correctly to generate appropriate responses based\n",
      "on retrieved information.  8. **Privacy and Security**:    - Handling sensitive\n",
      "information in the retrieval process requires stringent security measures to\n",
      "prevent data leaks or unauthorized access.  9. **Cost**:    - The computational\n",
      "resources needed for both retrieval and generation can be costly, especially for\n",
      "real-time applications.  10. **Evaluation and Metrics**:     - Traditional\n",
      "metrics for evaluating language models might not fully capture the effectiveness\n",
      "of RAG systems, necessitating new evaluation frameworks.  11. **User\n",
      "Experience**:     - Users might experience inconsistencies if the system fails\n",
      "to retrieve relevant information or if the generation does not align well with\n",
      "the retrieved data.  Addressing these challenges requires ongoing research and\n",
      "development in areas like natural language processing, information retrieval,\n",
      "and machine learning, as well as careful system design and implementation.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function and print the result\n",
    "llm_response = call_llm_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJH2__0iTUr1"
   },
   "source": [
    "# 3.Advanced RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awyjcn35jFiy"
   },
   "source": [
    "## 3.1.Vector search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kD8_758kkq3h"
   },
   "source": [
    "### Search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCBbY4qLc8qh"
   },
   "outputs": [],
   "source": [
    "def find_best_match(text_input, records):\n",
    "    best_score = 0\n",
    "    best_record = None\n",
    "    for record in records:\n",
    "        current_score = calculate_cosine_similarity(text_input, record)\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_record = record\n",
    "    return best_score, best_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RG1iM-U33OCg"
   },
   "outputs": [],
   "source": [
    "best_similarity_score, best_matching_record = find_best_match(query, db_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PLa9NQ4Cm_YQ",
    "outputId": "2a3ab188-4d6c-4036-90a6-bd721e4130cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "While Retrieval Augmented Generation (RAG) offers substantial benefits, it also\n",
      "comes with its challenges.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A60QoOA3jf9j"
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yIUh-38knHLI",
    "outputId": "f3e87248-1927-4240-db3b-acefc3d8c940"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cosine Similarity Score: 0.059\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PQopW_FSjBSr",
    "outputId": "af3c9342-5f7b-4866-b6b3-87f337038488"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What store offers rags? :  While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.\n",
      "Enhanced Similarity:, 0.641\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Similarity\n",
    "response = best_matching_record\n",
    "print(query,\": \", response)\n",
    "similarity_score = calculate_enhanced_similarity(query, best_matching_record)\n",
    "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51fZC6Oe2G9E"
   },
   "source": [
    "### Augmented input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dcnK7OGx5e6"
   },
   "outputs": [],
   "source": [
    "augmented_input=query+\": \"+best_matching_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T3uk-91x049J",
    "outputId": "07365a18-9685-41be-8205-121241a88b0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "What store offers rags?: While Retrieval Augmented Generation (RAG) offers\n",
      "substantial benefits, it also comes with its challenges.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(augmented_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFDF6hbi2LF9"
   },
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJC-mA5ftxFU",
    "outputId": "797bfa22-0987-44b3-cd6f-295a75c18a6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "Here are some of the **challenges** associated with Retrieval Augmented\n",
      "Generation (RAG):  1. **Quality of Retrieval**:     - The effectiveness of RAG\n",
      "heavily depends on the quality of the retrieval system. If the system retrieves\n",
      "irrelevant or low-quality documents, the generated output will be compromised.\n",
      "2. **Latency**:    - Adding a retrieval step increases the time required to\n",
      "generate responses, which can be a significant issue for applications requiring\n",
      "real-time interaction.  3. **Scalability**:    - As the corpus of documents\n",
      "grows, the retrieval process can become computationally expensive, affecting the\n",
      "scalability of the system.  4. **Relevance and Context Understanding**:    -\n",
      "Ensuring that the retrieved documents are not only relevant but also provide the\n",
      "right context for the query can be challenging. Misinterpretation or lack of\n",
      "context can lead to incorrect or misleading outputs.  5. **Data Privacy and\n",
      "Security**:    - When dealing with sensitive information, ensuring that the\n",
      "retrieval system does not expose or misuse private data is crucial.  6.\n",
      "**Integration Complexity**:    - Integrating RAG into existing systems can be\n",
      "complex, requiring adjustments in architecture, data pipelines, and possibly the\n",
      "user interface.  7. **Overfitting to Training Data**:    - If the retrieval\n",
      "system is trained on a specific dataset, it might not generalize well to new,\n",
      "unseen data or queries outside its training scope.  8. **Maintenance and\n",
      "Updates**:    - Keeping the document corpus up-to-date and relevant requires\n",
      "ongoing maintenance, which can be resource-intensive.  9. **Bias in Retrieval**:\n",
      "- There's a risk of bias in the documents retrieved, which can perpetuate or\n",
      "even amplify existing biases in the data.  10. **Evaluation Metrics**:     -\n",
      "Traditional metrics for evaluating language models might not be sufficient for\n",
      "RAG systems, where the quality of retrieval and generation needs to be assessed\n",
      "together.  11. **Cold Start Problem**:     - For new topics or domains where\n",
      "there's little to no existing documentation, RAG systems might struggle to\n",
      "provide accurate or useful information.  12. **User Experience**:     - Users\n",
      "might not understand why certain information is being presented or might find\n",
      "the system's responses less coherent if the retrieval step introduces unrelated\n",
      "content.  Addressing these challenges often requires a combination of advanced\n",
      "retrieval techniques, better model architectures, and careful system design to\n",
      "balance between retrieval accuracy, generation quality, and operational\n",
      "efficiency.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function and print the result\n",
    "llm_response = call_llm_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7djpPBpm0M2"
   },
   "source": [
    "## 3.2.Index-based search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyDUhy_1lBfT"
   },
   "source": [
    "### Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wRarT_fym2XC",
    "outputId": "da06ab68-e631-49da-dd58-c892facd6c05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def setup_vectorizer(records):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(records)\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "def find_best_match(query, vectorizer, tfidf_matrix):\n",
    "    query_tfidf = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_tfidf, tfidf_matrix)\n",
    "    best_index = similarities.argmax()  # Get the index of the highest similarity score\n",
    "    best_score = similarities[0, best_index]\n",
    "    return best_score, best_index\n",
    "\n",
    "vectorizer, tfidf_matrix = setup_vectorizer(db_records)\n",
    "\n",
    "best_similarity_score, best_index = find_best_match(query, vectorizer, tfidf_matrix)\n",
    "best_matching_record = db_records[best_index]\n",
    "\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jt3iBtJFj4sa"
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UoNUQqx5j3r4",
    "outputId": "f11dc99f-0692-4401-c531-a8f019d35849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cosine Similarity Score: 0.245\n",
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity\n",
    "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vg910Mhuj3sO",
    "outputId": "a257738d-f895-4df4-ebb2-14e5c923ef71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What store offers rags? :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
      "Enhanced Similarity:, 0.612\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Similarity\n",
    "response = best_matching_record\n",
    "print(query,\": \", response)\n",
    "similarity_score = calculate_enhanced_similarity(query, response)\n",
    "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubm0DTxKeqR9"
   },
   "source": [
    "Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SbokQ2eacHjM",
    "outputId": "5f755934-578f-4064-8c29-449084a69bee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ability    access  accuracy  accurate     adapt  additional  advancement  \\\n",
      "0   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "1   0.000000  0.000000  0.000000  0.216364  0.000000    0.000000     0.000000   \n",
      "2   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "3   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "4   0.000000  0.000000  0.000000  0.000000  0.000000    0.236479     0.000000   \n",
      "5   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "6   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "7   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "8   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "9   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "10  0.186734  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "11  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "12  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "13  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "14  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "15  0.000000  0.172624  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "16  0.000000  0.317970  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "17  0.000000  0.000000  0.000000  0.206861  0.000000    0.000000     0.000000   \n",
      "18  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "19  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "20  0.000000  0.000000  0.000000  0.000000  0.275802    0.000000     0.000000   \n",
      "21  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "22  0.000000  0.174772  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "23  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "24  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "25  0.000000  0.000000  0.228743  0.000000  0.000000    0.000000     0.000000   \n",
      "26  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.173327   \n",
      "27  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "\n",
      "      advice  algorithms    allows  ...    vector  vectorized      when  \\\n",
      "0   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "1   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "2   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "3   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "4   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "5   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "6   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "7   0.000000    0.220687  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "8   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "9   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "10  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "11  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.295573   \n",
      "12  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "13  0.000000    0.000000  0.000000  ...  0.200131     0.00000  0.000000   \n",
      "14  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "15  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "16  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "17  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "18  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "19  0.244401    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "20  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "21  0.000000    0.000000  0.291503  ...  0.000000     0.00000  0.000000   \n",
      "22  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "23  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "24  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "25  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "26  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "27  0.000000    0.000000  0.000000  ...  0.307719     0.34589  0.000000   \n",
      "\n",
      "       where     which    while     wide      with    within   without  \n",
      "0   0.000000  0.000000  0.00000  0.00000  0.000000  0.260582  0.000000  \n",
      "1   0.000000  0.000000  0.00000  0.00000  0.160278  0.000000  0.000000  \n",
      "2   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "3   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "4   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "5   0.000000  0.247710  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "6   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "7   0.000000  0.179053  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "8   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "9   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "10  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "11  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "12  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "13  0.000000  0.182517  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "14  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "15  0.189283  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "16  0.000000  0.000000  0.00000  0.00000  0.258278  0.000000  0.000000  \n",
      "17  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "18  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "19  0.217430  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "20  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "21  0.000000  0.000000  0.00000  0.00000  0.192110  0.000000  0.291503  \n",
      "22  0.000000  0.000000  0.00000  0.21541  0.141963  0.000000  0.000000  \n",
      "23  0.000000  0.000000  0.32932  0.00000  0.217033  0.000000  0.000000  \n",
      "24  0.000000  0.000000  0.00000  0.00000  0.134513  0.000000  0.000000  \n",
      "25  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "26  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "27  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "\n",
      "[28 rows x 297 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def setup_vectorizer(records):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(records)\n",
    "\n",
    "    # Convert the TF-IDF matrix to a DataFrame for display purposes\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(tfidf_df)\n",
    "\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "vectorizer, tfidf_matrix = setup_vectorizer(db_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dABZ12Bkugtt"
   },
   "source": [
    "### Augmented input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1w4wppuA4eNn"
   },
   "outputs": [],
   "source": [
    "augmented_input=query+\": \"+best_matching_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNozI65K4e7u",
    "outputId": "780d7f5c-9b0c-460d-d858-5c671d089268"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "What store offers rags?: A RAG vector store is a database or dataset that\n",
      "contains vectorized data points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(augmented_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hU998zkD4hpD"
   },
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uy9X-l_Iugtt",
    "outputId": "a262ae24-1134-42c7-b666-3281a8437c9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "It seems like there might be a bit of confusion in your question. If you're\n",
      "asking about physical rags, like cleaning rags, you can find them at:  -\n",
      "**Hardware stores** like Home Depot or Lowe's. - **Supermarkets** like Walmart\n",
      "or Target. - **Automotive stores** like AutoZone or O'Reilly Auto Parts. -\n",
      "**Dollar stores** or discount stores.  However, if you're referring to \"RAG\" in\n",
      "the context of **Retrieval-Augulated Generation** (RAG) in machine learning or\n",
      "AI, where \"RAG\" stands for a system or model that uses vectorized data for\n",
      "retrieval and generation tasks:  - **RAG vector stores** are not physical stores\n",
      "but rather databases or systems like:   - **Pinecone**   - **FAISS** (Facebook\n",
      "AI Similarity Search)   - **Elasticsearch** with vector capabilities   -\n",
      "**Milvus**   - **Qdrant**  These are used to store and retrieve vector\n",
      "embeddings for applications in natural language processing, recommendation\n",
      "systems, and more. If you need more information on how these systems work or how\n",
      "to implement them, feel free to ask!\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function and print the result\n",
    "llm_response = call_llm_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWEvzcDHTX6i"
   },
   "source": [
    "# 4.Modular RAG\n",
    "\n",
    "Modular RAG can combine methods. For example:\n",
    "\n",
    "**keyword search**:Searches through each document to find the one that best matches the keyword(s).\n",
    "\n",
    "**vector search**: Searches through each document and calculates similarity.\n",
    "\n",
    "**indexed search**: Uses a precomputed index (TF-IDF matrix) to compute cosine similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18wmqwJd4o62"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class RetrievalComponent:\n",
    "    def __init__(self, method='vector'):\n",
    "        self.method = method\n",
    "        if self.method == 'vector' or self.method == 'indexed':\n",
    "            self.vectorizer = TfidfVectorizer()\n",
    "            self.tfidf_matrix = None\n",
    "\n",
    "    def fit(self, records):\n",
    "      self.documents = records  # Initialize self.documents here\n",
    "      if self.method == 'vector' or self.method == 'indexed':\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(records)\n",
    "\n",
    "    def retrieve(self, query):\n",
    "        if self.method == 'keyword':\n",
    "            return self.keyword_search(query)\n",
    "        elif self.method == 'vector':\n",
    "            return self.vector_search(query)\n",
    "        elif self.method == 'indexed':\n",
    "            return self.indexed_search(query)\n",
    "\n",
    "    def keyword_search(self, query):\n",
    "        best_score = 0\n",
    "        best_record = None\n",
    "        query_keywords = set(query.lower().split())\n",
    "        for index, doc in enumerate(self.documents):\n",
    "            doc_keywords = set(doc.lower().split())\n",
    "            common_keywords = query_keywords.intersection(doc_keywords)\n",
    "            score = len(common_keywords)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_record = self.documents[index]\n",
    "        return best_record\n",
    "\n",
    "    def vector_search(self, query):\n",
    "        query_tfidf = self.vectorizer.transform([query])\n",
    "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
    "        best_index = similarities.argmax()\n",
    "        return db_records[best_index]\n",
    "\n",
    "    def indexed_search(self, query):\n",
    "        # Assuming the tfidf_matrix is precomputed and stored\n",
    "        query_tfidf = self.vectorizer.transform([query])\n",
    "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
    "        best_index = similarities.argmax()\n",
    "        return db_records[best_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qHm4saJ8cGk"
   },
   "source": [
    "### Modular RAG Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_kvhIOdY8amp",
    "outputId": "866c0ee3-6a07-4ecf-db5e-f87b981d0d81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "retrieval = RetrievalComponent(method='vector')  # Choose from 'keyword', 'vector', 'indexed'\n",
    "retrieval.fit(db_records)\n",
    "best_matching_record = retrieval.retrieve(query)\n",
    "\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgxXdqzvkYDk"
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "COyYme4IkYDx",
    "outputId": "975f0aaa-789e-4f55-f57f-f73bbeb36f67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cosine Similarity Score: 0.245\n",
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity\n",
    "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0YRTpIpzkYDx",
    "outputId": "8c5aa149-4ed8-494e-caca-5b126685d954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What store offers rags? :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
      "Enhanced Similarity: 0.6123305785907626\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Similarity\n",
    "response = best_matching_record\n",
    "print(query,\": \", response)\n",
    "similarity_score = calculate_enhanced_similarity(query, response)\n",
    "print(\"Enhanced Similarity:\", similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TaQa7Dc7JwT"
   },
   "source": [
    "### Augmented Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-hKjhIU7Jwg"
   },
   "outputs": [],
   "source": [
    "augmented_input=query+ \" \"+ best_matching_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZhSO-fyZ7Jwg",
    "outputId": "81bb498b-d110-43de-c6bd-c019caa67a76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "What store offers rags? A RAG vector store is a database or dataset that\n",
      "contains vectorized data points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(augmented_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkyYx_MC7Jwg"
   },
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-V3srRHW7Jwh",
    "outputId": "1c9a6217-2130-405a-a7f0-1f50791338c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "If you're referring to **RAG** in the context of **vector stores**:  - **RAG**\n",
      "stands for **Retrieval-Augmented Generation**. It's a technique used in natural\n",
      "language processing where a model retrieves relevant information from a database\n",
      "(like a vector store) to augment its generation capabilities.   - **Vector\n",
      "Stores** or **Vector Databases** are used to store embeddings or vectors, which\n",
      "are numerical representations of data points (like text, images, etc.). Here are\n",
      "some examples of services or tools that can be used as vector stores:    -\n",
      "**Pinecone**: A managed vector database service designed for similarity search\n",
      "and vector operations.      - **FAISS (Facebook AI Similarity Search)**: An\n",
      "open-source library for efficient similarity search and clustering of dense\n",
      "vectors.    - **Milvus**: An open-source vector database for embedding\n",
      "similarity search and AI applications.    - **Elasticsearch** with vector search\n",
      "capabilities: While primarily known for text search, it can be configured to\n",
      "handle vector similarity searches.    - **Annoy (Approximate Nearest Neighbors\n",
      "Oh Yeah)**: Another library for finding nearest neighbors in large datasets.\n",
      "- **HNSW (Hierarchical Navigable Small World)**: An algorithm implemented in\n",
      "libraries like NMSLIB for approximate nearest neighbor search.  If you're\n",
      "looking for physical stores that sell rags, here are some common places:  -\n",
      "**Hardware Stores** like Home Depot or Lowe's. - **Grocery Stores** often have\n",
      "cleaning supplies sections. - **Dollar Stores** like Dollar Tree or Family\n",
      "Dollar. - **Automotive Supply Stores** for shop rags or microfiber cloths. -\n",
      "**Online Retailers** like Amazon, Walmart, or Target.  Please clarify if you\n",
      "meant something different by \"rags\" or if you're looking for a specific type of\n",
      "vector store or database for RAG applications.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function and print the result\n",
    "llm_response = call_llm_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
